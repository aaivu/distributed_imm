{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-08T20:12:32.311187Z",
     "start_time": "2025-03-08T20:12:29.401153Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pandas as pd\n",
    "\n",
    "from d_imm.imm_model import DistributedIMM"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set Java environment variable if needed\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk1.8.0_261\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"C:\\\\Users\\\\saadha\\\\Desktop\\\\FYP-code\\\\GITHUB\\\\distributed-imm\\\\d-imm-python\\\\version-1\\\\venv\\\\Scripts\\\\python.exe\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"C:\\\\Users\\\\saadha\\\\Desktop\\\\FYP-code\\\\GITHUB\\\\distributed-imm\\\\d-imm-python\\\\version-1\\\\venv\\\\Scripts\\\\python.exe\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T20:12:32.318556Z",
     "start_time": "2025-03-08T20:12:32.313191Z"
    }
   },
   "id": "1059671c73eec2d5",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KMeansIrisExample\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "    # .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    # .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T20:12:42.026549Z",
     "start_time": "2025-03-08T20:12:32.319564Z"
    }
   },
   "id": "1cb6d646eb22c860",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T20:12:42.033186Z",
     "start_time": "2025-03-08T20:12:42.027627Z"
    }
   },
   "id": "4f4968f2137853d7",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the Iris dataset from the UCI Machine Learning Repository\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "column_names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\"]\n",
    "iris_df = pd.read_csv(url, header=None, names=column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T20:12:43.168313Z",
     "start_time": "2025-03-08T20:12:42.035571Z"
    }
   },
   "id": "b3b9d0a575578fdd",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Convert the pandas DataFrame to a Spark DataFrame\n",
    "df_1 = spark.createDataFrame(iris_df)\n",
    "\n",
    "# Stack the dataset 5 times row-wise\n",
    "df = df_1\n",
    "for _ in range(0):  # Repeat 4 more times to stack 5 times total\n",
    "    df = df.union(df_1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T20:12:46.391822Z",
     "start_time": "2025-03-08T20:12:43.169816Z"
    }
   },
   "id": "35079de4f208c08d",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Assemble features into a single vector column\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "feature_df = assembler.transform(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T20:12:47.868350Z",
     "start_time": "2025-03-08T20:12:46.393923Z"
    }
   },
   "id": "24e16358ad51cdac",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set up the KMeans model (k=3 for the three species in the Iris dataset)\n",
    "kmeans = KMeans().setK(3).setSeed(1).setFeaturesCol(\"features\")\n",
    "\n",
    "# Fit the model\n",
    "model = kmeans.fit(feature_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T20:13:20.892876Z",
     "start_time": "2025-03-08T20:12:47.869482Z"
    }
   },
   "id": "3a96f2c1e3cdb9c6",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 'fit' method\n",
      "Cluster centers: [array([6.85384615, 3.07692308, 5.71538462, 2.05384615]), array([5.006, 3.418, 1.464, 0.244]), array([5.88360656, 2.74098361, 4.38852459, 1.43442623])]\n",
      "Sample of clustered data:\n",
      "+-----------------+----------+\n",
      "|         features|prediction|\n",
      "+-----------------+----------+\n",
      "|[5.1,3.5,1.4,0.2]|         1|\n",
      "|[4.9,3.0,1.4,0.2]|         1|\n",
      "|[4.7,3.2,1.3,0.2]|         1|\n",
      "|[4.6,3.1,1.5,0.2]|         1|\n",
      "|[5.0,3.6,1.4,0.2]|         1|\n",
      "+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "Time taken to build the histogram: 0 minutes and 48.76 seconds\n",
      "Histogram: [[Split(feature_index=0, threshold=np.float64(4.55), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(4.65), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(4.85), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(4.95), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(5.05), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(5.15), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(5.25), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(5.35), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(5.45), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(5.55), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(5.65), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(5.75), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(5.85), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(5.95), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(6.05), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(6.15), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(6.25), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(6.35), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(6.45), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(6.55), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(6.65), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(6.75), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(6.85), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(6.95), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(7.05), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(7.15), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(7.25), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(7.35), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(7.5), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(7.65), categories=None, is_continuous=True), Split(feature_index=0, threshold=np.float64(7.800000000000001), categories=None, is_continuous=True)], [Split(feature_index=1, threshold=np.float64(2.1), categories=None, is_continuous=True), Split(feature_index=1, threshold=np.float64(2.25), categories=None, is_continuous=True), Split(feature_index=1, threshold=np.float64(2.3499999999999996), categories=None, is_continuous=True), Split(feature_index=1, threshold=np.float64(2.45), categories=None, is_continuous=True), Split(feature_index=1, threshold=np.float64(2.55), categories=None, is_continuous=True), Split(feature_index=1, threshold=np.float64(2.6500000000000004), categories=None, is_continuous=True), Split(feature_index=1, threshold=np.float64(2.75), categories=None, is_continuous=True), Split(feature_index=1, threshold=np.float64(2.8499999999999996), categories=None, is_continuous=True), Split(feature_index=1, threshold=np.float64(2.95), categories=None, is_continuous=True), Split(feature_index=1, threshold=np.float64(3.05), categories=None, is_continuous=True), Split(feature_index=1, threshold=np.float64(3.1500000000000004), categories=None, is_continuous=True), Split(feature_index=1, threshold=np.float64(3.25), categories=None, is_continuous=True), Split(feature_index=1, threshold=np.float64(3.3499999999999996), categories=None, is_continuous=True), Split(feature_index=1, threshold=np.float64(3.45), categories=None, is_continuous=True), Split(feature_index=1, threshold=np.float64(3.55), categories=None, is_continuous=True), Split(feature_index=1, threshold=np.float64(3.6500000000000004), categories=None, is_continuous=True), Split(feature_index=1, threshold=np.float64(3.75), categories=None, is_continuous=True), Split(feature_index=1, threshold=np.float64(3.8499999999999996), categories=None, is_continuous=True), Split(feature_index=1, threshold=np.float64(3.95), categories=None, is_continuous=True), Split(feature_index=1, threshold=np.float64(4.05), categories=None, is_continuous=True), Split(feature_index=1, threshold=np.float64(4.15), categories=None, is_continuous=True), Split(feature_index=1, threshold=np.float64(4.300000000000001), categories=None, is_continuous=True)], [Split(feature_index=2, threshold=np.float64(1.25), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(1.35), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(1.45), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(1.55), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(1.65), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(1.7999999999999998), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(2.45), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(3.15), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(3.4), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(3.55), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(3.6500000000000004), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(3.75), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(3.8499999999999996), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(4.05), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(4.15), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(4.25), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(4.45), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(4.55), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(4.65), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(4.75), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(4.85), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(4.95), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(5.05), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(5.15), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(5.25), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(5.35), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(5.45), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(5.55), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(5.65), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(5.85), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(6.05), categories=None, is_continuous=True), Split(feature_index=2, threshold=np.float64(6.35), categories=None, is_continuous=True)], [Split(feature_index=3, threshold=np.float64(0.15000000000000002), categories=None, is_continuous=True), Split(feature_index=3, threshold=np.float64(0.25), categories=None, is_continuous=True), Split(feature_index=3, threshold=np.float64(0.35), categories=None, is_continuous=True), Split(feature_index=3, threshold=np.float64(0.45), categories=None, is_continuous=True), Split(feature_index=3, threshold=np.float64(0.55), categories=None, is_continuous=True), Split(feature_index=3, threshold=np.float64(0.8), categories=None, is_continuous=True), Split(feature_index=3, threshold=np.float64(1.05), categories=None, is_continuous=True), Split(feature_index=3, threshold=np.float64(1.15), categories=None, is_continuous=True), Split(feature_index=3, threshold=np.float64(1.25), categories=None, is_continuous=True), Split(feature_index=3, threshold=np.float64(1.35), categories=None, is_continuous=True), Split(feature_index=3, threshold=np.float64(1.45), categories=None, is_continuous=True), Split(feature_index=3, threshold=np.float64(1.55), categories=None, is_continuous=True), Split(feature_index=3, threshold=np.float64(1.65), categories=None, is_continuous=True), Split(feature_index=3, threshold=np.float64(1.75), categories=None, is_continuous=True), Split(feature_index=3, threshold=np.float64(1.85), categories=None, is_continuous=True), Split(feature_index=3, threshold=np.float64(1.95), categories=None, is_continuous=True), Split(feature_index=3, threshold=np.float64(2.05), categories=None, is_continuous=True), Split(feature_index=3, threshold=np.float64(2.1500000000000004), categories=None, is_continuous=True), Split(feature_index=3, threshold=np.float64(2.25), categories=None, is_continuous=True), Split(feature_index=3, threshold=np.float64(2.3499999999999996), categories=None, is_continuous=True), Split(feature_index=3, threshold=np.float64(2.45), categories=None, is_continuous=True)]]\n",
      "Building node at depth 0 with 150 samples\n",
      "Finding best split using histogram thresholds\n",
      "Time to collect worker results: 0 minutes and 15.41 seconds\n",
      "Best split: Feature 2, Threshold 2.45, Mistakes 0\n",
      "Time taken to find best split: 0 minutes and 15.44 seconds\n",
      "Splitting on feature 2 at threshold 2.45 with mistakes 0\n",
      "Building node at depth 1 with 50 samples\n",
      "Building node at depth 1 with 100 samples\n",
      "Finding best split using histogram thresholds\n",
      "Time to collect worker results: 0 minutes and 14.99 seconds\n",
      "Best split: Feature 2, Threshold 5.15, Mistakes 5\n",
      "Time taken to find best split: 0 minutes and 15.02 seconds\n",
      "Splitting on feature 2 at threshold 5.15 with mistakes 5\n",
      "Building node at depth 2 with 66 samples\n",
      "Building node at depth 2 with 34 samples\n",
      "Tree building completed.\n",
      "Time taken to fill stats: 0 minutes and 51.06 seconds\n"
     ]
    }
   ],
   "source": [
    "d_imm_tree = DistributedIMM(spark,3,verbose=4).fit(feature_df,model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T20:16:22.824990Z",
     "start_time": "2025-03-08T20:13:20.895035Z"
    }
   },
   "id": "7a28edd8637b21cd",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "Tree plot saved as 'iris_imm_tree_3.png' and displayed.\n"
     ]
    }
   ],
   "source": [
    "# Extract feature names from the VectorAssembler\n",
    "feature_names = assembler.getInputCols()\n",
    "\n",
    "# Print the feature names to confirm\n",
    "print(\"Feature names:\", feature_names)\n",
    "\n",
    "# Plot the tree using the dynamically retrieved feature names\n",
    "try:\n",
    "    d_imm_tree.plot(filename=\"iris_imm_tree_3\", feature_names=feature_names, view=True)\n",
    "    print(\"Tree plot saved as 'iris_imm_tree_3.png' and displayed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while plotting the tree: {e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T20:16:24.086427Z",
     "start_time": "2025-03-08T20:16:22.831868Z"
    }
   },
   "id": "cf51cb88b442234a",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 'feature_importance' method\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0, 0, 2, 0]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_imm_tree.feature_importance()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T20:16:24.254849Z",
     "start_time": "2025-03-08T20:16:24.185549Z"
    }
   },
   "id": "de2afbf1171561cd",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMM tree saved successfully to iris_imm_tree.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Define the file path to save the tree\n",
    "tree_save_path = \"iris_imm_tree.pkl\"\n",
    "\n",
    "# Save the trained IMM tree to a file\n",
    "with open(tree_save_path, \"wb\") as f:\n",
    "    pickle.dump(d_imm_tree.tree, f)\n",
    "\n",
    "print(f\"IMM tree saved successfully to {tree_save_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T20:16:24.270380Z",
     "start_time": "2025-03-08T20:16:24.257007Z"
    }
   },
   "id": "86ec21d73d2439ef",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Node' object has no attribute 'tree'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m     loaded_d_imm_tree \u001B[38;5;241m=\u001B[39m pickle\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Use the tree directly\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mloaded_d_imm_tree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtree\u001B[49m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Node' object has no attribute 'tree'"
     ]
    }
   ],
   "source": [
    "# Load IMM tree\n",
    "with open(tree_save_path, \"rb\") as f:\n",
    "    loaded_d_imm_tree = pickle.load(f)\n",
    "\n",
    "# Use the tree directly\n",
    "print(loaded_d_imm_tree.tree)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T20:16:26.058959Z",
     "start_time": "2025-03-08T20:16:24.275357Z"
    }
   },
   "id": "3521916d4064feb9",
   "execution_count": 13
  },
  {
   "cell_type": "raw",
   "source": [
    "TESTING THE \"fill_stats_distributed\" METHOD"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8485f0f030c03101"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Node Stats: Samples = 150\n",
      "Left Child Stats: Samples = 50 Mistakes = 0\n",
      "Right Child Stats: Samples = 100\n",
      "Right-Left Child Stats: Samples = 66 Mistakes = 5\n",
      "Right-Right Child Stats: Samples = 34 Mistakes = 0\n"
     ]
    }
   ],
   "source": [
    "from d_imm.imm_model import Node\n",
    "\n",
    "# Root Node\n",
    "root_node = Node()\n",
    "root_node.feature = 2  # petal length (cm) column index\n",
    "root_node.value = 1.9  # Threshold for split\n",
    "\n",
    "# Left Child - Leaf Node\n",
    "root_node.left = Node()\n",
    "root_node.left.value = 1  # Cluster label\n",
    "# root_node.left.samples = 50\n",
    "# root_node.left.mistakes = 0\n",
    "\n",
    "# Right Child - Internal Node\n",
    "root_node.right = Node()\n",
    "root_node.right.feature = 2  # petal length (cm) column index\n",
    "root_node.right.value = 5.1  # Threshold for split\n",
    "\n",
    "# Right-Left Child - Leaf Node\n",
    "root_node.right.left = Node()\n",
    "root_node.right.left.value = 2  # Cluster label\n",
    "# root_node.right.left.samples = 66\n",
    "# root_node.right.left.mistakes = 5\n",
    "\n",
    "# Right-Right Child - Leaf Node\n",
    "root_node.right.right = Node()\n",
    "root_node.right.right.value = 0  # Cluster label\n",
    "# root_node.right.right.samples = 34\n",
    "# root_node.right.right.mistakes = 0\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "d_imm = DistributedIMM(spark, k=3, verbose=1)\n",
    "\n",
    "clustered_data = model.transform(feature_df).select(\"features\", \"prediction\")\n",
    "clustered_data_vector = clustered_data.withColumn(\"features_array\", vector_to_array(\"features\"))\n",
    "\n",
    "# Test the fill_stats_distributed method with the manually created tree\n",
    "d_imm.fill_stats_distributed(root_node,clustered_data_vector)\n",
    "\n",
    "# Print the results for verification\n",
    "print(\"Root Node Stats: Samples =\", root_node.samples)\n",
    "print(\"Left Child Stats: Samples =\", root_node.left.samples, \"Mistakes =\", root_node.left.mistakes)\n",
    "print(\"Right Child Stats: Samples =\", root_node.right.samples)\n",
    "print(\"Right-Left Child Stats: Samples =\", root_node.right.left.samples, \"Mistakes =\", root_node.right.left.mistakes)\n",
    "print(\"Right-Right Child Stats: Samples =\", root_node.right.right.samples, \"Mistakes =\", root_node.right.right.mistakes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T20:17:30.593327Z",
     "start_time": "2025-03-08T20:16:35.801342Z"
    }
   },
   "id": "49ab78c79365f4fd",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "TESTING FEATURE IMPORTANCE "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5d9308a659743da"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 'feature_importance' method\n",
      "[0, 0, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "feature_imp = d_imm.feature_importance()\n",
    "print(feature_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T20:17:30.603021Z",
     "start_time": "2025-03-08T20:17:30.596674Z"
    }
   },
   "id": "e146bd7eb2805c4b",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<d_imm.imm_model.Node at 0x19d42d66f90>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_imm_tree.tree"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T20:17:30.615502Z",
     "start_time": "2025-03-08T20:17:30.606252Z"
    }
   },
   "id": "62a06902f513897c",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "TESTING SCORE FUNCTIONS"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab14e284fc4e108c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1231, in main\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1067, in read_udfs\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 547, in read_single_udf\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 110, in wrap_scalar_pandas_udf\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\types.py\", line 65, in to_arrow_type\n    import pyarrow as pa\nModuleNotFoundError: No module named 'pyarrow'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPythonException\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Compute Score and Surrogate Score with udf\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m score_value \u001B[38;5;241m=\u001B[39m \u001B[43md_imm_tree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscore\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeature_df\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m surrogate_score_value \u001B[38;5;241m=\u001B[39m d_imm_tree\u001B[38;5;241m.\u001B[39msurrogate_score(feature_df)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Print results\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\FYP-code\\distributed_imm\\d_imm\\imm_model.py:401\u001B[0m, in \u001B[0;36mDistributedIMM.score\u001B[1;34m(self, x_data)\u001B[0m\n\u001B[0;32m    399\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mfloat\u001B[39m(np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(np\u001B[38;5;241m.\u001B[39marray(feat) \u001B[38;5;241m-\u001B[39m np\u001B[38;5;241m.\u001B[39marray(center)) \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    400\u001B[0m cost_df \u001B[38;5;241m=\u001B[39m predicted\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msquared_distance\u001B[39m\u001B[38;5;124m\"\u001B[39m, squared_distance(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures_array\u001B[39m\u001B[38;5;124m\"\u001B[39m), col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcluster_mean\u001B[39m\u001B[38;5;124m\"\u001B[39m)))\n\u001B[1;32m--> 401\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcost_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magg\u001B[49m\u001B[43m(\u001B[49m\u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msquared_distance\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\Desktop\\FYP-code\\distributed_imm\\venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1263\u001B[0m, in \u001B[0;36mDataFrame.collect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1243\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001B[39;00m\n\u001B[0;32m   1244\u001B[0m \n\u001B[0;32m   1245\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1260\u001B[0m \u001B[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001B[39;00m\n\u001B[0;32m   1261\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1262\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SCCallSiteSync(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc):\n\u001B[1;32m-> 1263\u001B[0m     sock_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollectToPython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1264\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001B[1;32m~\\Desktop\\FYP-code\\distributed_imm\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32m~\\Desktop\\FYP-code\\distributed_imm\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    181\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[0;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[0;32m    183\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[0;32m    184\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[1;32m--> 185\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mPythonException\u001B[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1231, in main\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1067, in read_udfs\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 547, in read_single_udf\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 110, in wrap_scalar_pandas_udf\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\types.py\", line 65, in to_arrow_type\n    import pyarrow as pa\nModuleNotFoundError: No module named 'pyarrow'\n"
     ]
    }
   ],
   "source": [
    "# Compute Score and Surrogate Score with udf\n",
    "score_value = d_imm_tree.score(feature_df)\n",
    "surrogate_score_value = d_imm_tree.surrogate_score(feature_df)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n===== Distributed IMM Score Testing =====\")\n",
    "print(f\"Score (K-Means Cost): {score_value:.4f}\")\n",
    "print(f\"Surrogate Score (K-Means Surrogate Cost): {surrogate_score_value:.4f}\")\n",
    "\n",
    "# Validate that surrogate score is greater than or equal to k-means score\n",
    "assert surrogate_score_value >= score_value, \"Surrogate score should be greater than or equal to the normal score.\"\n",
    "\n",
    "print(\"✅ Score function tests passed successfully.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T20:17:46.928179Z",
     "start_time": "2025-03-08T20:17:30.619048Z"
    }
   },
   "id": "1b42e11312c13dd1",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1231, in main\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1067, in read_udfs\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 547, in read_single_udf\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 110, in wrap_scalar_pandas_udf\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\types.py\", line 65, in to_arrow_type\n    import pyarrow as pa\nModuleNotFoundError: No module named 'pyarrow'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPythonException\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Compute Score and Surrogate Score with spark sql\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m score_value \u001B[38;5;241m=\u001B[39m \u001B[43md_imm_tree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscore_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeature_df\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m surrogate_score_value \u001B[38;5;241m=\u001B[39m d_imm_tree\u001B[38;5;241m.\u001B[39msurrogate_score_sql(feature_df)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Print results\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\FYP-code\\distributed_imm\\d_imm\\imm_model.py:423\u001B[0m, in \u001B[0;36mDistributedIMM.score_sql\u001B[1;34m(self, x_data)\u001B[0m\n\u001B[0;32m    420\u001B[0m predicted \u001B[38;5;241m=\u001B[39m predicted\u001B[38;5;241m.\u001B[39mjoin(cluster_means, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    421\u001B[0m joined \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_join_exploded(predicted, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeatures_array\u001B[39m\u001B[38;5;124m\"\u001B[39m, cluster_means, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcluster_mean\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    422\u001B[0m total_cost \u001B[38;5;241m=\u001B[39m \u001B[43mjoined\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroupBy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprediction\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magg\u001B[49m\u001B[43m(\u001B[49m\u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msquared_distance\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malias\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtotal_cost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m--> 423\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magg\u001B[49m\u001B[43m(\u001B[49m\u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtotal_cost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    424\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m total_cost\n",
      "File \u001B[1;32m~\\Desktop\\FYP-code\\distributed_imm\\venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1263\u001B[0m, in \u001B[0;36mDataFrame.collect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1243\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001B[39;00m\n\u001B[0;32m   1244\u001B[0m \n\u001B[0;32m   1245\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1260\u001B[0m \u001B[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001B[39;00m\n\u001B[0;32m   1261\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1262\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SCCallSiteSync(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc):\n\u001B[1;32m-> 1263\u001B[0m     sock_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollectToPython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1264\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001B[1;32m~\\Desktop\\FYP-code\\distributed_imm\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32m~\\Desktop\\FYP-code\\distributed_imm\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    181\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[0;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[0;32m    183\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[0;32m    184\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[1;32m--> 185\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mPythonException\u001B[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1231, in main\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1067, in read_udfs\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 547, in read_single_udf\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 110, in wrap_scalar_pandas_udf\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\sql\\pandas\\types.py\", line 65, in to_arrow_type\n    import pyarrow as pa\nModuleNotFoundError: No module named 'pyarrow'\n"
     ]
    }
   ],
   "source": [
    "# Compute Score and Surrogate Score with spark sql\n",
    "score_value = d_imm_tree.score_sql(feature_df)\n",
    "surrogate_score_value = d_imm_tree.surrogate_score_sql(feature_df)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n===== Distributed IMM Score Testing =====\")\n",
    "print(f\"Score (K-Means Cost): {score_value:.4f}\")\n",
    "print(f\"Surrogate Score (K-Means Surrogate Cost): {surrogate_score_value:.4f}\")\n",
    "\n",
    "# Validate that surrogate score is greater than or equal to k-means score\n",
    "assert surrogate_score_value >= score_value, \"Surrogate score should be greater than or equal to the normal score.\"\n",
    "\n",
    "print(\"✅ Score function tests passed successfully.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T20:18:46.412385Z",
     "start_time": "2025-03-08T20:18:32.159413Z"
    }
   },
   "id": "ed535c9822b6a11f",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c8a48d7d8da42c04"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
