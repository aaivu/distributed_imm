{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-09T14:48:15.615874Z",
     "start_time": "2025-02-09T14:48:09.107219Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pandas as pd\n",
    "\n",
    "from d_imm.imm_model import DistributedIMM"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set Java environment variable if needed\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk1.8.0_261\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"C:\\\\Users\\\\saadha\\\\Desktop\\\\FYP-code\\\\GITHUB\\\\distributed-imm\\\\d-imm-python\\\\version-1\\\\venv\\\\Scripts\\\\python.exe\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"C:\\\\Users\\\\saadha\\\\Desktop\\\\FYP-code\\\\GITHUB\\\\distributed-imm\\\\d-imm-python\\\\version-1\\\\venv\\\\Scripts\\\\python.exe\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T15:08:28.892599Z",
     "start_time": "2025-02-09T15:08:28.776892Z"
    }
   },
   "id": "1059671c73eec2d5",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KMeansIrisExample\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T15:10:30.927209Z",
     "start_time": "2025-02-09T15:08:29.337766Z"
    }
   },
   "id": "1cb6d646eb22c860",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T15:10:31.692689Z",
     "start_time": "2025-02-09T15:10:31.632594Z"
    }
   },
   "id": "4f4968f2137853d7",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Imported successfully', 'Imported successfully']\n"
     ]
    }
   ],
   "source": [
    "def test_import(iterator):\n",
    "    try:\n",
    "        from d_imm.splitters.cut_finder import get_all_mistakes\n",
    "        return [\"Imported successfully\"]\n",
    "    except Exception as e:\n",
    "        return [f\"Import failed: {str(e)}\"]\n",
    "\n",
    "rdd = sc.parallelize(range(10), 2)  # Create an RDD with 2 partitions\n",
    "result = rdd.mapPartitions(test_import).collect()\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T15:10:57.501795Z",
     "start_time": "2025-02-09T15:10:31.704826Z"
    }
   },
   "id": "ef1a7e5737105b65",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the Iris dataset from the UCI Machine Learning Repository\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "column_names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\"]\n",
    "iris_df = pd.read_csv(url, header=None, names=column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T15:15:34.234901Z",
     "start_time": "2025-02-09T15:15:31.357416Z"
    }
   },
   "id": "b3b9d0a575578fdd",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Convert the pandas DataFrame to a Spark DataFrame\n",
    "df_1 = spark.createDataFrame(iris_df)\n",
    "\n",
    "# Stack the dataset 5 times row-wise\n",
    "df = df_1\n",
    "for _ in range(0):  # Repeat 4 more times to stack 5 times total\n",
    "    df = df.union(df_1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T15:15:43.260705Z",
     "start_time": "2025-02-09T15:15:34.240874Z"
    }
   },
   "id": "35079de4f208c08d",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Assemble features into a single vector column\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "feature_df = assembler.transform(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T15:15:47.519395Z",
     "start_time": "2025-02-09T15:15:43.262889Z"
    }
   },
   "id": "24e16358ad51cdac",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set up the KMeans model (k=3 for the three species in the Iris dataset)\n",
    "kmeans = KMeans().setK(3).setSeed(1).setFeaturesCol(\"features\")\n",
    "\n",
    "# Fit the model\n",
    "model = kmeans.fit(feature_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T15:16:54.945867Z",
     "start_time": "2025-02-09T15:15:47.522885Z"
    }
   },
   "id": "3a96f2c1e3cdb9c6",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 'fit' method\n",
      "Cluster centers: [array([6.85384615, 3.07692308, 5.71538462, 2.05384615]), array([5.006, 3.418, 1.464, 0.244]), array([5.88360656, 2.74098361, 4.38852459, 1.43442623])]\n",
      "Sample of clustered data:\n",
      "+-----------------+----------+\n",
      "|         features|prediction|\n",
      "+-----------------+----------+\n",
      "|[5.1,3.5,1.4,0.2]|         1|\n",
      "|[4.9,3.0,1.4,0.2]|         1|\n",
      "|[4.7,3.2,1.3,0.2]|         1|\n",
      "|[4.6,3.1,1.5,0.2]|         1|\n",
      "|[5.0,3.6,1.4,0.2]|         1|\n",
      "+-----------------+----------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 29.0 failed 1 times, most recent failure: Lost task 4.0 in stage 29.0 (TID 178) (LAPTOP-P8U0OKKO executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1225, in main\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 594, in read_int\n    length = stream.read(4)\n             ^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Python311\\Lib\\socket.py\", line 706, in readinto\n    return self._sock.recv_into(b)\n           ^^^^^^^^^^^^^^^^^^^^^^^\nTimeoutError: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1225, in main\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 594, in read_int\n    length = stream.read(4)\n             ^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Python311\\Lib\\socket.py\", line 706, in readinto\n    return self._sock.recv_into(b)\n           ^^^^^^^^^^^^^^^^^^^^^^^\nTimeoutError: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m d_imm_3 \u001B[38;5;241m=\u001B[39m \u001B[43mDistributedIMM\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspark\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeature_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\FYP-code\\distributed_imm\\d_imm\\imm_model.py:133\u001B[0m, in \u001B[0;36mDistributedIMM.fit\u001B[1;34m(self, x_data, kmeans_model)\u001B[0m\n\u001B[0;32m    122\u001B[0m split_finder \u001B[38;5;241m=\u001B[39m DecisionTreeSplitFinder(\n\u001B[0;32m    123\u001B[0m     num_features\u001B[38;5;241m=\u001B[39mfeature_count,\n\u001B[0;32m    124\u001B[0m     is_continuous\u001B[38;5;241m=\u001B[39mvalid_cols,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    129\u001B[0m     seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m  \u001B[38;5;66;03m# Default Value = 42\u001B[39;00m\n\u001B[0;32m    130\u001B[0m )\n\u001B[0;32m    132\u001B[0m \u001B[38;5;66;03m# Find splits using the split finder\u001B[39;00m\n\u001B[1;32m--> 133\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhistogram \u001B[38;5;241m=\u001B[39m \u001B[43msplit_finder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfind_splits\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_rdd\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclustered_rdd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;66;03m# self.histogram_broadcast = self.spark.sparkContext.broadcast(self.histogram)\u001B[39;00m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtree \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_tree(clustered_data, valid_centers\u001B[38;5;241m=\u001B[39mvalid_centers, valid_cols\u001B[38;5;241m=\u001B[39mvalid_cols)\n",
      "File \u001B[1;32m~\\Desktop\\FYP-code\\distributed_imm\\d_imm\\histogram\\histogram.py:59\u001B[0m, in \u001B[0;36mDecisionTreeSplitFinder.find_splits\u001B[1;34m(self, input_rdd)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfind_splits\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_rdd: RDD) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[0;32m     53\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;124;03m    Public method to find splits for decision tree calculation.\u001B[39;00m\n\u001B[0;32m     55\u001B[0m \n\u001B[0;32m     56\u001B[0m \u001B[38;5;124;03m    :param input_rdd: RDD of Instance objects.\u001B[39;00m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;124;03m    :return: A 2D list (list of lists) of Split objects [featureIndex -> list of Splits].\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_find_splits_by_sorting\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_rdd\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\FYP-code\\distributed_imm\\d_imm\\histogram\\histogram.py:278\u001B[0m, in \u001B[0;36mDecisionTreeSplitFinder._find_splits_by_sorting\u001B[1;34m(self, sampled_input_rdd)\u001B[0m\n\u001B[0;32m    275\u001B[0m categorical_features \u001B[38;5;241m=\u001B[39m [i \u001B[38;5;28;01mfor\u001B[39;00m i, cont \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_continuous) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m cont]\n\u001B[0;32m    277\u001B[0m \u001B[38;5;66;03m# 2. Handle continuous features\u001B[39;00m\n\u001B[1;32m--> 278\u001B[0m continuous_splits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_find_splits_for_continuous_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43msampled_input_rdd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontinuous_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    280\u001B[0m \u001B[38;5;66;03m# 3. Handle categorical features\u001B[39;00m\n\u001B[0;32m    281\u001B[0m categorical_splits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_find_splits_for_categorical_features(sampled_input_rdd, categorical_features)\n",
      "File \u001B[1;32m~\\Desktop\\FYP-code\\distributed_imm\\d_imm\\histogram\\histogram.py:328\u001B[0m, in \u001B[0;36mDecisionTreeSplitFinder._find_splits_for_continuous_features\u001B[1;34m(self, sampled_input_rdd, continuous_features)\u001B[0m\n\u001B[0;32m    319\u001B[0m feature_aggregates \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    320\u001B[0m     feature_value_pairs\n\u001B[0;32m    321\u001B[0m     \u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m x: (x[\u001B[38;5;241m0\u001B[39m], x[\u001B[38;5;241m1\u001B[39m]))  \u001B[38;5;66;03m# (featureIndex, featureValue)\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    324\u001B[0m     \u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m x: (x[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m], (x[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m1\u001B[39m], x[\u001B[38;5;241m1\u001B[39m])))  \u001B[38;5;66;03m# (featureIndex, (featureValue, count))\u001B[39;00m\n\u001B[0;32m    325\u001B[0m )\n\u001B[0;32m    327\u001B[0m \u001B[38;5;66;03m# Collect as map: { featureIndex -> list of (featureValue, count) }\u001B[39;00m\n\u001B[1;32m--> 328\u001B[0m feature_value_counts \u001B[38;5;241m=\u001B[39m \u001B[43mfeature_aggregates\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroupByKey\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmapValues\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollectAsMap\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    330\u001B[0m \u001B[38;5;66;03m# Now compute splits for each continuous feature\u001B[39;00m\n\u001B[0;32m    331\u001B[0m continuous_splits \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[1;32m~\\Desktop\\FYP-code\\distributed_imm\\venv\\Lib\\site-packages\\pyspark\\rdd.py:3457\u001B[0m, in \u001B[0;36mRDD.collectAsMap\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   3429\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcollectAsMap\u001B[39m(\u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple[K, V]]\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[K, V]:\n\u001B[0;32m   3430\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   3431\u001B[0m \u001B[38;5;124;03m    Return the key-value pairs in this RDD to the master as a dictionary.\u001B[39;00m\n\u001B[0;32m   3432\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3455\u001B[0m \u001B[38;5;124;03m    4\u001B[39;00m\n\u001B[0;32m   3456\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 3457\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\Desktop\\FYP-code\\distributed_imm\\venv\\Lib\\site-packages\\pyspark\\rdd.py:1833\u001B[0m, in \u001B[0;36mRDD.collect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1831\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SCCallSiteSync(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext):\n\u001B[0;32m   1832\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1833\u001B[0m     sock_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPythonRDD\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollectAndServe\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jrdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrdd\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1834\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jrdd_deserializer))\n",
      "File \u001B[1;32m~\\Desktop\\FYP-code\\distributed_imm\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32m~\\Desktop\\FYP-code\\distributed_imm\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 179\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    180\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    181\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[1;32m~\\Desktop\\FYP-code\\distributed_imm\\venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[1;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 29.0 failed 1 times, most recent failure: Lost task 4.0 in stage 29.0 (TID 178) (LAPTOP-P8U0OKKO executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1225, in main\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 594, in read_int\n    length = stream.read(4)\n             ^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Python311\\Lib\\socket.py\", line 706, in readinto\n    return self._sock.recv_into(b)\n           ^^^^^^^^^^^^^^^^^^^^^^^\nTimeoutError: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1225, in main\n  File \"C:\\spark-3.5.3-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 594, in read_int\n    length = stream.read(4)\n             ^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Python311\\Lib\\socket.py\", line 706, in readinto\n    return self._sock.recv_into(b)\n           ^^^^^^^^^^^^^^^^^^^^^^^\nTimeoutError: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "d_imm_3 = DistributedIMM(spark,3,verbose=4,mode=3).fit(feature_df,model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-09T15:17:52.332077Z",
     "start_time": "2025-02-09T15:16:54.951768Z"
    }
   },
   "id": "7a28edd8637b21cd",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d_imm_2 = DistributedIMM(spark,3,verbose=4,mode=2).fit(feature_df,model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8bf2997d3edb9f6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d_imm_1 = DistributedIMM(spark,3,verbose=4,mode=1).fit(feature_df,model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-09T06:00:59.447829Z"
    }
   },
   "id": "4f3d49151c5c9fdf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d_imm_0 = DistributedIMM(spark,3,verbose=4,mode=0).fit(feature_df,model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "619c889e1d0e3ce8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "Tree plot saved as 'iris_imm_tree_3.png' and displayed.\n"
     ]
    }
   ],
   "source": [
    "# Extract feature names from the VectorAssembler\n",
    "feature_names = assembler.getInputCols()\n",
    "\n",
    "# Print the feature names to confirm\n",
    "print(\"Feature names:\", feature_names)\n",
    "\n",
    "# Plot the tree using the dynamically retrieved feature names\n",
    "try:\n",
    "    d_imm_3.plot(filename=\"iris_imm_tree_3\", feature_names=feature_names, view=True)\n",
    "    print(\"Tree plot saved as 'iris_imm_tree_3.png' and displayed.\")\n",
    "    \n",
    "    d_imm_2.plot(filename=\"iris_imm_tree_2\", feature_names=feature_names, view=True)\n",
    "    print(\"Tree plot saved as 'iris_imm_tree_2.png' and displayed.\")\n",
    "\n",
    "    d_imm_0.plot(filename=\"iris_imm_tree_1\", feature_names=feature_names, view=True)\n",
    "    print(\"Tree plot saved as 'iris_imm_tree_1.png' and displayed.\")\n",
    "\n",
    "    d_imm_1.plot(filename=\"iris_imm_tree_0\", feature_names=feature_names, view=True)\n",
    "    print(\"Tree plot saved as 'iris_imm_tree_0.png' and displayed.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while plotting the tree: {e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-06T06:01:06.248177Z",
     "start_time": "2025-02-06T06:01:00.236958Z"
    }
   },
   "id": "cf51cb88b442234a",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "Tree plot saved as 'iris_imm_tree.png' and displayed.\n"
     ]
    }
   ],
   "source": [
    "# Extract feature names from the VectorAssembler\n",
    "feature_names = assembler.getInputCols()\n",
    "\n",
    "# Print the feature names to confirm\n",
    "print(\"Feature names:\", feature_names)\n",
    "\n",
    "# Plot the tree using the dynamically retrieved feature names\n",
    "try:\n",
    "    d_imm_0.plot(filename=\"iris_imm_tree_2\", feature_names=feature_names, view=True)\n",
    "    print(\"Tree plot saved as 'iris_imm_tree.png' and displayed.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while plotting the tree: {e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T17:11:13.023776Z",
     "start_time": "2025-02-04T17:11:07.708238Z"
    }
   },
   "id": "3545fb63941218",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 'feature_importance' method\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0, 0, 2, 0]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_imm_0.feature_importance()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T17:10:57.858263Z",
     "start_time": "2025-02-04T17:10:57.776485Z"
    }
   },
   "id": "de2afbf1171561cd",
   "execution_count": 17
  },
  {
   "cell_type": "raw",
   "source": [
    "TESTING THE \"fill_stats_distributed\" METHOD"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8485f0f030c03101"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from d_imm.imm_model import Node\n",
    "\n",
    "# Root Node\n",
    "root_node = Node()\n",
    "root_node.feature = 2  # petal length (cm) column index\n",
    "root_node.value = 1.9  # Threshold for split\n",
    "\n",
    "# Left Child - Leaf Node\n",
    "root_node.left = Node()\n",
    "root_node.left.value = 1  # Cluster label\n",
    "# root_node.left.samples = 50\n",
    "# root_node.left.mistakes = 0\n",
    "\n",
    "# Right Child - Internal Node\n",
    "root_node.right = Node()\n",
    "root_node.right.feature = 2  # petal length (cm) column index\n",
    "root_node.right.value = 5.1  # Threshold for split\n",
    "\n",
    "# Right-Left Child - Leaf Node\n",
    "root_node.right.left = Node()\n",
    "root_node.right.left.value = 2  # Cluster label\n",
    "# root_node.right.left.samples = 66\n",
    "# root_node.right.left.mistakes = 5\n",
    "\n",
    "# Right-Right Child - Leaf Node\n",
    "root_node.right.right = Node()\n",
    "root_node.right.right.value = 0  # Cluster label\n",
    "# root_node.right.right.samples = 34\n",
    "# root_node.right.right.mistakes = 0\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "d_imm = DistributedIMM(spark, k=3, verbose=1)\n",
    "\n",
    "clustered_data = model.transform(feature_df).select(\"features\", \"prediction\")\n",
    "clustered_data_vector = clustered_data.withColumn(\"features_array\", vector_to_array(\"features\"))\n",
    "\n",
    "# Test the fill_stats_distributed method with the manually created tree\n",
    "d_imm.fill_stats_distributed(root_node,clustered_data_vector)\n",
    "\n",
    "# Print the results for verification\n",
    "print(\"Root Node Stats: Samples =\", root_node.samples)\n",
    "print(\"Left Child Stats: Samples =\", root_node.left.samples, \"Mistakes =\", root_node.left.mistakes)\n",
    "print(\"Right Child Stats: Samples =\", root_node.right.samples)\n",
    "print(\"Right-Left Child Stats: Samples =\", root_node.right.left.samples, \"Mistakes =\", root_node.right.left.mistakes)\n",
    "print(\"Right-Right Child Stats: Samples =\", root_node.right.right.samples, \"Mistakes =\", root_node.right.right.mistakes)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49ab78c79365f4fd",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "TESTING FEATURE IMPORTANCE "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5d9308a659743da"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "feature_imp = d_imm.feature_importance()\n",
    "print(feature_imp)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e146bd7eb2805c4b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1b42e11312c13dd1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
